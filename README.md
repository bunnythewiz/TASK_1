# DBNet for Scanned Receipt Text Localization

![PyTorch](https://img.shields.io/badge/PyTorch-2.7.1-EE4C2C?style=flat&logo=pytorch)
![Python](https://img.shields.io/badge/Python-3.13-3776AB?style=flat&logo=python)

Real-time text detection on scanned receipts using Differentiable Binarization Network (DBNet) with ResNet-18 backbone and Feature Pyramid Network.

---

## ğŸ¯ Overview

This repository implements **DBNet (Differentiable Binarization Network)** for text localization in scanned receipt images. DBNet uses a novel differentiable binarization approach that makes the entire text detection pipeline end-to-end trainable, eliminating the need for post-processing steps.

### Key Innovation: Differentiable Binarization

Traditional text detection methods apply fixed thresholds during post-processing, which cannot be optimized during training. DBNet introduces:

- **Adaptive Thresholding**: Pixel-wise learnable thresholds
- **Differentiable Approximation**: Uses a differentiable step function
- **End-to-End Training**: Binarization becomes part of the optimization process

### Architecture

```
Input Image
    â†“
ResNet-18 (Backbone) - Pretrained on ImageNet
    â†“
FPN (Neck) - Multi-scale feature fusion (256 channels)
    â†“
DBHead - Generates 3 maps:
    â”œâ”€â”€ Probability Map (Shrink Map)
    â”œâ”€â”€ Threshold Map (Adaptive)
    â””â”€â”€ Binary Map (Differentiable Binarization)
```

---

## âœ¨ Features

- âœ… **Real-time Performance**: 43 FPS on standard GPU
- âœ… **End-to-End Trainable**: No post-processing required
- âœ… **Multi-Scale Detection**: FPN handles varying text sizes
- âœ… **Adaptive Thresholding**: Handles varying image quality
- âœ… **Lightweight**: ResNet-18 backbone (11.2M parameters)
- âœ… **Robust Training**: 92.3% loss reduction over 50 epochs
- âœ… **High Accuracy**: 99.7% training accuracy, 0.971 IoU

---

## ğŸ“Š Results

### Training Performance

| Metric | Epoch 1 | Epoch 50 | Improvement |
|--------|---------|----------|-------------|
| **Training Loss** | 2.536 | 0.196 | 92.3% â†“ |
| **Accuracy** | 51.2% | 99.7% | +48.5% |
| **IoU (Shrink Map)** | 0.059 | 0.971 | +0.912 |
| **Shrink Map Loss** | 0.716 | 0.028 | 96.1% â†“ |
| **Threshold Map Loss** | 0.080 | 0.017 | 78.8% â†“ |
| **Binary Map Loss** | 0.553 | 0.017 | 97.0% â†“ |

### Training Phases

1. **Rapid Convergence (Epochs 1-10)**: 68.7% loss reduction
2. **Refinement (Epochs 11-30)**: 40.6% additional reduction
3. **Fine-Tuning (Epochs 31-50)**: 55.3% further improvement

**Total Training Time**: 1.62 hours (50 epochs on GPU)

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- CUDA 11.8+ (for GPU training)
- PyTorch 2.0+

### Clone Repository

```
git clone https://github.com/bunnythewiz/TASK_1.git
cd TASK_1
```

### Create Virtual Environment

```
# Using conda
conda create -n dbnet python=3.13
conda activate dbnet

# Or using venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
```

### Install Dependencies

```
pip install -r requirement.txt

# Install PyTorch (adjust for your CUDA version)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Package Requirements

```
torch>=2.0.0
torchvision>=0.15.0
opencv-python>=4.8.0
numpy>=1.24.0
Pillow>=10.0.0
pyyaml>=6.0
tensorboard>=2.13.0
shapely>=2.0.0
scipy>=1.11.0
imgaug>=0.4.0
tqdm>=4.65.0
```

---

## ğŸ“ Dataset Preparation

### ICDAR 2015 Dataset

1. **Download Dataset**
   ```
   # Download from: https://rrc.cvc.uab.es/?ch=4&com=downloads
   wget https://rrc.cvc.uab.es/downloads/ch4_training_images.zip
   wget https://rrc.cvc.uab.es/downloads/ch4_training_localization_transcription_gt.zip
   ```

2. **Organize Data**
   ```
   datasets/
   â”œâ”€â”€ train/
   â”‚   â”œâ”€â”€ img001.jpg
   â”‚   â”œâ”€â”€ gt_img001.txt
   â”‚   â”œâ”€â”€ img002.jpg
   â”‚   â”œâ”€â”€ gt_img002.txt
   â”‚   â””â”€â”€ ...
   â””â”€â”€ test/
       â”œâ”€â”€ img001.jpg
       â”œâ”€â”€ gt_img001.txt
       â””â”€â”€ ...
   ```

3. **Generate File Lists**
   ```
   bash generate_lists.sh
   ```

   This creates:
   - `train.txt`: Training image paths and annotations
   - `test.txt`: Test image paths and annotations

### Annotation Format

Ground truth files (`gt_*.txt`) should contain one line per text instance:

```
x1,y1,x2,y2,x3,y3,x4,y4,transcription
```

Example:
```
377,117,463,117,465,130,378,130,COFFEE
```

---

## ğŸ‹ï¸ Training

### Single GPU Training

```
bash singlel_gpu_train.sh
```

Or directly:

```
python train.py \
  --config config/icdar2015_resnet18_FPN_DBhead_polyLR.yaml \
  --resume_checkpoint output/checkpoint/model_latest.pth  # Optional
```

### Training Configuration

Edit `config/icdar2015_resnet18_FPN_DBhead_polyLR.yaml`:

```
arch:
  backbone:
    type: resnet18
    pretrained: True
  neck:
    type: FPN
    inner_channels: 256
  head:
    type: DBHead
    k: 50

trainer:
  epochs: 50
  log_iter: 10
  save_interval: 10

optimizer:
  type: Adam
  lr: 0.001
  weight_decay: 0.0

loss:
  type: DBLoss
  alpha: 1.0
  beta: 10
  ohem_ratio: 3
```

### Monitor Training

```
tensorboard --logdir output/DBNet_Receipt_Detection/
```

## ğŸ“‚ Project Structure

```
TASK_1/
â”œâ”€â”€ base/                          # Base classes
â”‚   â”œâ”€â”€ base_dataset.py           # Dataset base class
â”‚   â””â”€â”€ base_trainer.py           # Trainer base class
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ icdar2015_resnet18_FPN_DBhead_polyLR.yaml
â”‚   â”œâ”€â”€ icdar2015_resnet50_FPN_DBhead_polyLR.yaml
â”‚   â””â”€â”€ open_dataset_resnet18_FPN_DBhead_polyLR.yaml
â”œâ”€â”€ data_loader/                   # Data loading and augmentation
â”‚   â”œâ”€â”€ dataset.py                # ICDAR dataset loader
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ augment.py            # Data augmentation
â”‚       â”œâ”€â”€ make_shrink_map.py    # Shrink map generation
â”‚       â””â”€â”€ make_border_map.py    # Border map generation
â”œâ”€â”€ models/                        # Model architectures
â”‚   â”œâ”€â”€ backbone/                 # Backbone networks
â”‚   â”‚   â”œâ”€â”€ resnet.py             # ResNet-18/50
â”‚   â”‚   â””â”€â”€ resnest.py            # ResNeSt
â”‚   â”œâ”€â”€ neck/                     # Neck modules
â”‚   â”‚   â””â”€â”€ FPN.py                # Feature Pyramid Network
â”‚   â”œâ”€â”€ head/                     # Detection heads
â”‚   â”‚   â””â”€â”€ DBHead.py             # Differentiable Binarization Head
â”‚   â”œâ”€â”€ losses/                   # Loss functions
â”‚   â”‚   â”œâ”€â”€ DB_loss.py            # DBNet multi-component loss
â”‚   â”‚   â””â”€â”€ basic_loss.py         # Basic loss components
â”‚   â””â”€â”€ model.py                  # Model builder
â”œâ”€â”€ post_processing/               # Post-processing
â”‚   â””â”€â”€ seg_detector_representer.py  # Convert predictions to boxes
â”œâ”€â”€ utils/                         # Utilities
â”‚   â”œâ”€â”€ metrics.py                # Evaluation metrics
â”‚   â”œâ”€â”€ schedulers.py             # Learning rate schedulers
â”‚   â””â”€â”€ util.py                   # General utilities
â”œâ”€â”€ output/                        # Training outputs
â”‚   â””â”€â”€ DBNet_Receipt_Detection/
â”‚       â”œâ”€â”€ checkpoint/           # Model checkpoints
â”‚       â”‚   â”œâ”€â”€ model_best.pth
â”‚       â”‚   â””â”€â”€ model_latest.pth
â”‚       â””â”€â”€ tensorboard/          # TensorBoard logs
â”œâ”€â”€ train.py                       # Training script
â”œâ”€â”€ eval.py                        # Evaluation script
â”œâ”€â”€ predict.py                     # Inference script
â”œâ”€â”€ requirement.txt                # Python dependencies
â”œâ”€â”€ environment.yml                # Conda environment
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ LICENSE.md                     # Apache 2.0 License
```
